import { supabase } from '@/app/lib/db/supabase';
import { log } from '../helpers';

// L√≥gica para tomar las URLs no scrapeadas de la base de datos Supabase
export async function takeUrls(): Promise<{ id: string; url: string }[]> {
  // Configuracion de paginaci√≥n
  const BATCH_SIZE = 100; // N√∫mero de URLs a recuperar por lote
  let from = 0;
  let to = BATCH_SIZE - 1;
  const MAX = 2000; // L√≠mite m√°ximo de URLs a procesar

  // Array para almacenar todas las URLs sacadas de la base de datos
  const allUrls: { id: string; url: string }[] = [];

  // Bucle para paginar a trav√©s de las URLs no scrapeadas
  while (true) {
    try {
      const { data: urlsBatch, error } = await supabase
        .from('scraped_urls')
        .select('id, url')
        .eq('is_scraped', false)
        .range(from, to);

      if (error) {
        log(`Error fetching URLs (range ${from}-${to}): ${error.message}`, 'error');
        break;
      }

      // Si no hay m√°s URLs, salimos del bucle
      if (!urlsBatch || urlsBatch.length === 0) break;

      // Si hemos alcanzado el l√≠mite m√°ximo, salimos del bucle
      if (allUrls.length >= MAX) {
        log(`‚ö†Ô∏è L√≠mite m√°ximo de ${MAX} URLs alcanzado, deteniendo lectura.`, 'warn');
        break;
      }

      // A√±adimos el lote actual al array principal
      allUrls.push(...urlsBatch);

      log(`üß© Cargadas ${urlsBatch.length} URLs (total acumulado: ${allUrls.length})`, 'info');

      // Mover al siguiente bloque
      from += BATCH_SIZE;
      to += BATCH_SIZE;
    } catch (error) {
      // Manejar errores generales
      log(`Error fetching URLs (range ${from}-${to}): ${error}`, 'error');
      throw new Error(`Error fetching URLs (range ${from}-${to}): ${error}`);
    }
  }

  return allUrls;
}
